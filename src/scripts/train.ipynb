{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5imzg7fEJeLM"
      },
      "source": [
        "# Traffic prediction modeling with GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3urCm8nvnFrY",
        "outputId": "f48826d4-8756-4e8d-bf7f-5f4c4ae1146b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'# Only colab stuff\\ntry:\\n  from google.colab import drive\\n  drive.mount(\\'/content/gdrive\\')\\n  import torch\\n  import os\\n  print(\"PyTorch has version {}\".format(torch.__version__))\\n  if \\'IS_GRADESCOPE_ENV\\' not in os.environ:\\n    !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\\n    !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\\n    !pip install torch-geometric\\n    !pip install ogb\\n\\n    os.chdir(\"./gdrive/MyDrive/MLG_cloned_repo/src/scripts\")\\nexcept:\\n  print(\"Not in colab!\")\\n  os.chdir(\"./src/scripts\") '"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# Only colab stuff\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import torch\n",
        "  import os\n",
        "  print(\"PyTorch has version {}\".format(torch.__version__))\n",
        "  if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "    !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "    !pip install torch-geometric\n",
        "    !pip install ogb\n",
        "\n",
        "    os.chdir(\"./gdrive/MyDrive/MLG_cloned_repo/src/scripts\")\n",
        "except:\n",
        "  print(\"Not in colab!\")\n",
        "  os.chdir(\"./src/scripts\") '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8EIVXxkJeLQ"
      },
      "source": [
        "### Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE_usvT1JeLR",
        "outputId": "1834e75b-5bb7-4946-d73e-e1686375db3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.1+cpu\n",
            "Using cpu\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "#Custom scripts\n",
        "import modeling_utils #as modeling_utils \n",
        "import data_preparation #as data_preparation\n",
        "\n",
        "#Pytorch and PyG\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR \n",
        "from torch_geometric.nn import GATConv, GCNConv\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "print(torch.__version__)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "#Lets start at src location\n",
        "if os.path.exists(\"./src\"):\n",
        "  os.chdir(\"./src\")\n",
        "elif 'scripts' in os.getcwd():\n",
        "  os.chdir(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E29uodRcJeLU",
        "outputId": "e2ef3e6e-21b3-4aeb-aa6d-c8e101223284"
      },
      "outputs": [],
      "source": [
        "#%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PugM8CEeJeLV"
      },
      "source": [
        "### Constants and setting-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "W8Wtx_uTJeLW"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SummaryWriter' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m# Make a tensorboard writer\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39muse_tensorboard\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m---> 39\u001b[0m     writer \u001b[39m=\u001b[39m SummaryWriter()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'SummaryWriter' is not defined"
          ]
        }
      ],
      "source": [
        "# Constant config to use throughout\n",
        "config = {\n",
        "    'TRAIN_TEST_PROPORTION'             : (0.6, 0.1, 0.3),\n",
        "    'BATCH_SIZE'                        : 64,\n",
        "    'EPOCHS'                            : 700,\n",
        "    'WEIGHT_DECAY'                      : 5e-5,\n",
        "    'INITIAL_LR'                        : 1e-2,\n",
        "    'DROPOUT'                           : 0.2,\n",
        "    'ATTENTION_HEADS'                   : 8,\n",
        "    'RESULTS_DIR'                       : './runs/'+time.strftime(\"%m-%dT%H-%M-%S\")+'/',\n",
        "    'data_with_already_filled_gaps'     : True,\n",
        "    'counter_files_path'                : '../data/counters_filled_gaps/',                         # '../data/counters_temporal_data_2023-03-03T09-24-06/'\n",
        "    'counters_nontemporal_aggregated'   : '../data/counters_non_temporal_aggregated_data.csv',\n",
        "    'holidays_path'                     : '../data/holidays.csv',\n",
        "    'USE_HOLIDAY_FEATURES'              : True,\n",
        "    'USE_WEEKDAY_FEATURES'              : False,\n",
        "    'N_GRAPHS'                          : 10*24,\n",
        "    'F_IN'                              : 14*24,\n",
        "    'F_OUT'                             : 7*24,\n",
        "    'N_NODE'                            : 165,\n",
        "    'target_col'                        : 'Sum',\n",
        "    'use_tensorboard'                   : True,\n",
        "    'USE_GAT'                           : True, # if True use GAT, else use GCN\n",
        "    'USE_LSTM'                          : True, # if True use LSTM, else use GRU\n",
        "    'LSTM_LAYER_SIZES'                  : [128, 32],  \n",
        "    'GRU_LAYER_SIZES'                   : [512, 512, 64, 32],    \n",
        "    'USE_EARLY_STOPPING'                : False,\n",
        "    \"MIN_ITERATIONS_EARLY_STOPPING\"     : 30,\n",
        "    \"EARLY_STOPPING_TOLERANCE\"          : 10,\n",
        "    \"LOG_BASELINE\"                      : True, # if true outputs average rmse on computed on each batch,\n",
        "    \"DATA_DATE_SPLIT\"                   : datetime.datetime(2022, 10, 1, 0, 0, 0, tzinfo=datetime.timezone.utc)\n",
        "}\n",
        "\n",
        "# Set logging level\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "# Make a tensorboard writer\n",
        "if config[\"use_tensorboard\"]:\n",
        "    writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTedXzi9JeLY"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ69dl_7JeLZ"
      },
      "outputs": [],
      "source": [
        "class ST_GNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Spatio-Temporal Graph Neural Network which has options of using:\n",
        "    1) Normal neighbor aggregation OR attention mechanism\n",
        "    2) GRU temporal layer or LSTM\n",
        "    \"\"\" \n",
        "    def __init__(self, device, in_channels, out_channels, n_nodes, heads=None, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the ST-GNN model\n",
        "        :param in_channels Number of input channels\n",
        "        :param out_channels Number of output channels\n",
        "        :param n_nodes Number of nodes in the graph\n",
        "        :param heads Number of attention heads to use in graph\n",
        "        :param dropout Dropout probability on output of Graph Attention Network\n",
        "        \"\"\"\n",
        "        # Set up params\n",
        "        super(ST_GNN, self).__init__()\n",
        "        self.device = device\n",
        "        self.n_pred = out_channels\n",
        "        self.dropout = dropout\n",
        "        self.n_nodes = n_nodes\n",
        "        #self.n_preds = 9 TODO is this needed?\n",
        "        \n",
        "        # Init spatial part\n",
        "        if config['USE_GAT']:\n",
        "            self.heads = heads\n",
        "            self.gat = GATConv(in_channels=in_channels, out_channels=in_channels,\n",
        "                    heads=heads, dropout=0, concat=False)\n",
        "        else:\n",
        "            self.gcn = GCNConv(in_channels=in_channels, out_channels=in_channels, dropout=0, concat=False)\n",
        "\n",
        "        # Init temporal part\n",
        "        if config['USE_LSTM']:\n",
        "            self.lstms = []\n",
        "            for layer_index, layer_size in enumerate(config[\"LSTM_LAYER_SIZES\"]):\n",
        "                if layer_index == 0: input_size = self.n_nodes\n",
        "                else: input_size = config[\"LSTM_LAYER_SIZES\"][layer_index - 1]\n",
        "\n",
        "                lstm = torch.nn.LSTM(input_size=input_size, hidden_size=layer_size, num_layers=1, device = self.device)\n",
        "                for name, param in lstm.named_parameters():\n",
        "                    if 'bias' in name:\n",
        "                        torch.nn.init.constant_(param, 0.0)\n",
        "                    elif 'weight' in name:\n",
        "                        torch.nn.init.xavier_uniform_(param)\n",
        "                self.lstms.append(lstm)\n",
        "\n",
        "            # fully-connected neural network\n",
        "            self.linear = torch.nn.Linear(config[\"LSTM_LAYER_SIZES\"][-1], self.n_nodes*self.n_pred)\n",
        "        else:\n",
        "            self.grus = []\n",
        "            for layer_index, layer_size in enumerate(config[\"GRU_LAYER_SIZES\"]):\n",
        "                if layer_index == 0: input_size = self.n_nodes\n",
        "                else: input_size = config[\"GRU_LAYER_SIZES\"][layer_index - 1]\n",
        "\n",
        "                gru = torch.nn.GRU(input_size=input_size, hidden_size=layer_size, num_layers=1, device = self.device)\n",
        "                self.grus.append(gru)\n",
        "\n",
        "            # fully-connected neural network\n",
        "            self.linear = torch.nn.Linear(config[\"GRU_LAYER_SIZES\"][-1], self.n_nodes*self.n_pred)\n",
        "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "    def forward(self, data, device):\n",
        "        \"\"\"\n",
        "        Forward pass of the ST-GNN model\n",
        "        :param data Data to make a pass on\n",
        "        :param device Device to operate on\n",
        "        \"\"\"\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        if self.device == 'cpu':\n",
        "            x = torch.FloatTensor(x)\n",
        "        else:\n",
        "            x = torch.cuda.FloatTensor(x)\n",
        "\n",
        "        if config['USE_GAT']:\n",
        "            x = self.gat(x, edge_index)\n",
        "        else:\n",
        "            x = self.gcn(x, edge_index)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "\n",
        "        batch_size = data.num_graphs\n",
        "        n_node = int(data.num_nodes/batch_size)\n",
        "        x = torch.reshape(x, (batch_size, n_node, data.num_features))\n",
        "        x = torch.movedim(x, 2, 0)\n",
        "        if config[\"USE_LSTM\"]:\n",
        "            for lstm in self.lstms:\n",
        "                x, _ = lstm(x)\n",
        "        else:\n",
        "            for gru in self.grus:\n",
        "                x, _ = gru(x)\n",
        "\n",
        "\n",
        "        x = torch.squeeze(x[-1, :, :])\n",
        "        x = self.linear(x)\n",
        "\n",
        "        s = x.shape\n",
        "        x = torch.reshape(x, (s[0], self.n_nodes, self.n_pred))\n",
        "        x = torch.reshape(x, (s[0]*self.n_nodes, self.n_pred))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XikFEW33JeLb"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval(model, device, dataloader, type='', dim_vars=None, save_predictions=False):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    mae = 0\n",
        "    rmse = 0\n",
        "    baseline_rmse = 0\n",
        "    mape = 0\n",
        "    n = 0\n",
        "\n",
        "    # Evaluate model on all data\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch, device)\n",
        "            truth = batch.y.view(pred.shape)\n",
        "            if i == 0:\n",
        "                y_pred = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
        "                y_truth = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
        "            #truth = un_z_score(truth, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
        "            #pred = un_z_score(pred, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
        "            \n",
        "            # reshape predictions\n",
        "            y_pred[i, :pred.shape[0], :] = pred\n",
        "            y_truth[i, :pred.shape[0], :] = truth\n",
        "\n",
        "            # save y_prediction & true values for later analysis\n",
        "            if save_predictions:\n",
        "                modeling_utils.save_all_predictions(y_pred, y_truth, dim_vars, config['RESULTS_DIR'])\n",
        "\n",
        "            # calculate batch average (take info only from x and take mean)\n",
        "            pred_avg = torch.mean(batch.x[:,:config['F_IN']], axis=1, keepdim=True).repeat(1,config['F_OUT'])\n",
        "\n",
        "            # calculate loss\n",
        "            rmse += modeling_utils.RMSE(truth, pred)\n",
        "            baseline_rmse += modeling_utils.RMSE(truth, pred_avg)\n",
        "            mae += modeling_utils.MAE(truth, pred)\n",
        "            mape += modeling_utils.MAPE(truth, pred)\n",
        "\n",
        "            n += 1\n",
        "    rmse, mae, mape, baseline_rmse = rmse / n, mae / n, mape / n, baseline_rmse / n\n",
        "\n",
        "    logging.info(f'{type}, MAE: {round(int(mae),2)}, RMSE: {round(int(rmse),2)}, MAPE: {round(int(mape),2)}')\n",
        "\n",
        "    #get the average score for each metric in each batch\n",
        "    return rmse, mae, mape, baseline_rmse, y_pred, y_truth\n",
        "\n",
        "\n",
        "def epoch_train(model, device, dataloader, optimizer, loss_fn, epoch):    \n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    model.train()\n",
        "    for _, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = torch.squeeze(model(batch, device))\n",
        "        loss = loss_fn()(y_pred.float(), torch.squeeze(batch.y).float())\n",
        "        if config[\"use_tensorboard\"]:\n",
        "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # multiplicative decay\n",
        "        scheduler.step()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGNsWYRnJeLd"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F3SQDz9JeLd"
      },
      "outputs": [],
      "source": [
        "def model_train(train_dataloader, val_dataloader, config, device, save_test_results = False, test_dataloader = None, dim_vars = None):\n",
        "    \"\"\"\n",
        "    Train the ST-GAT model. Evaluate on validation dataset as you go.\n",
        "    :param train_dataloader Data loader of training dataset\n",
        "    :param val_dataloader Dataloader of val dataset\n",
        "    :param config configuration to use\n",
        "    :param device Device to evaluate on\n",
        "    \"\"\"\n",
        "\n",
        "    # Make the model. Each datapoint in the graph is 228x12: N x F (N = # nodes, F = time window)\n",
        "    in_channels=config['F_IN']\n",
        "    if config[\"USE_HOLIDAY_FEATURES\"]: \n",
        "        in_channels += 7 * data_preparation.number_of_countries_in_holiday_dataset(config)\n",
        "    if config[\"USE_WEEKDAY_FEATURES\"]:\n",
        "        in_channels += 1\n",
        "    \n",
        "    model = ST_GNN(\n",
        "        device = device,\n",
        "        in_channels=in_channels, \n",
        "        out_channels=config['F_OUT'], \n",
        "        n_nodes=config['N_NODE'], \n",
        "        heads=config['ATTENTION_HEADS'], \n",
        "        dropout=config['DROPOUT']\n",
        "    )\n",
        "    \n",
        "    logging.info(\"Model initialized\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['INITIAL_LR'], weight_decay=config['WEIGHT_DECAY'])\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=config['INITIAL_LR'], weight_decay=config['WEIGHT_DECAY'])\n",
        "    loss_fn = torch.nn.MSELoss\n",
        "    model.to(device)\n",
        "\n",
        "    # Early stopping variables\n",
        "    n_iteration_since_loss_improvment = 0\n",
        "    best_train_loss = 999999999999999999\n",
        "\n",
        "    # For every epoch, train the model on training dataset. Evaluate model on validation dataset\n",
        "    for epoch in range(config['EPOCHS']):\n",
        "        loss = epoch_train(model, device, train_dataloader, optimizer, loss_fn, epoch)\n",
        "        logging.info(f\"Loss: {loss:.3f}\")\n",
        "        if epoch % 5 == 0:\n",
        "            train_rmse, train_mae, train_mape, _, _, _ = eval(model, device, train_dataloader, 'Train')\n",
        "            val_rmse, val_mae, val_mape, _, _, _ = eval(model, device, val_dataloader, 'Valid')\n",
        "            if config[\"use_tensorboard\"]:\n",
        "                writer.add_scalar(f\"MAE/train\", train_mae, epoch)\n",
        "                writer.add_scalar(f\"RMSE/train\", train_rmse, epoch)\n",
        "                writer.add_scalar(f\"MAPE/train\", train_mape, epoch)\n",
        "                writer.add_scalar(f\"MAE/val\", val_mae, epoch)\n",
        "                writer.add_scalar(f\"RMSE/val\", val_rmse, epoch)\n",
        "                writer.add_scalar(f\"MAPE/val\", val_mape, epoch)\n",
        "        \n",
        "        if config[\"USE_EARLY_STOPPING\"]:\n",
        "          if loss < best_train_loss:\n",
        "            best_train_loss = loss\n",
        "            n_iteration_since_loss_improvment = 0\n",
        "          else: n_iteration_since_loss_improvment += 1\n",
        "\n",
        "          if epoch >= config[\"MIN_ITERATIONS_EARLY_STOPPING\"] and \\\n",
        "                n_iteration_since_loss_improvment >= config[\"EARLY_STOPPING_TOLERANCE\"]:\n",
        "            break\n",
        "    logging.info(\"All epochs done, finished training\")\n",
        "\n",
        "    if config[\"use_tensorboard\"]:\n",
        "        writer.flush()\n",
        "    # Save the model\n",
        "    os.mkdir(config[\"RESULTS_DIR\"])\n",
        "    torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"loss\": loss},\n",
        "            os.path.join(config[\"RESULTS_DIR\"], \"model.pt\")\n",
        "    )\n",
        "    \n",
        "    with open(os.path.join(config[\"RESULTS_DIR\"], \"config.json\"), \"w\") as fp:\n",
        "        json.dump(config, fp)\n",
        "\n",
        "    if save_test_results:\n",
        "        test_rmse, test_mae, test_mape, baseline_rmse, _, _ = eval(model, device, test_dataloader, 'Test', dim_vars, save_predictions=True)\n",
        "        logging.info(f\"Test RMSE:{test_rmse}\")\n",
        "        if config['LOG_BASELINE']:\n",
        "          logging.info(f\"Test BASELINE RMSE:{baseline_rmse}\")\n",
        "        results = {'MAE': test_mae.item(),\n",
        "                    'RMSE': test_rmse.item(),\n",
        "                    'MAPE': test_mape.item()}\n",
        "        with open(os.path.join(config[\"RESULTS_DIR\"], \"results.json\"), \"w\") as fp:\n",
        "            json.dump(results, fp)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP3A3akKJeLf"
      },
      "source": [
        "### Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "eYwcGOI2JeLf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\markoi\\AppData\\Local\\Temp\\ipykernel_21112\\553531720.py\", line 2, in <module>\n",
            "    importlib.reload(data_preparation)\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\importlib\\__init__.py\", line 169, in reload\n",
            "    _bootstrap._exec(spec, module)\n",
            "  File \"<frozen importlib._bootstrap>\", line 619, in _exec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"c:\\Users\\markoi\\Desktop\\TrafficPrediction_MLG_Project\\src\\scripts\\data_preparation.py\", line 10, in <module>\n",
            "    from sklearn.preprocessing import MinMaxScaler\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\sklearn\\__init__.py\", line 82, in <module>\n",
            "    from .base import clone\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\sklearn\\base.py\", line 17, in <module>\n",
            "    from .utils import _IS_32BIT\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 21, in <module>\n",
            "    from . import _joblib\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\sklearn\\utils\\_joblib.py\", line 8, in <module>\n",
            "    from joblib import logger\n",
            "ImportError: cannot import name 'logger' from 'joblib' (unknown location)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
            "    frames.append(self.format_record(r))\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
            "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
            "    pieces = self.included_pieces\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
            "    pos = scope_pieces.index(self.executing_piece)\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
            "    return only(\n",
            "  File \"c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
            "    raise NotOneValueFound('Expected one value, found 0')\n",
            "executing.executing.NotOneValueFound: Expected one value, found 0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "importlib.reload(data_preparation)\n",
        "importlib.reload(modeling_utils)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIYqn84HJeLg",
        "outputId": "0610bc84-7840-48f8-eb65-f75be9d1cd16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Preparing data...\n",
            "INFO:root:Holiday features successfully prepared\n",
            "INFO:root:Historical counter data successfully read\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(config[\u001b[39m'\u001b[39m\u001b[39mRESULTS_DIR\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mrsplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m)[\u001b[39m0\u001b[39m]):\n\u001b[0;32m      3\u001b[0m     os\u001b[39m.\u001b[39mmkdir(config[\u001b[39m'\u001b[39m\u001b[39mRESULTS_DIR\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mrsplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m2\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m dataset, dim_vars \u001b[39m=\u001b[39m data_preparation\u001b[39m.\u001b[39;49mprepare_pyg_dataset(config)\n\u001b[0;32m      6\u001b[0m train_g, val_g, test_g, train_vars, val_vars, test_vars \u001b[39m=\u001b[39m data_preparation\u001b[39m.\u001b[39msplit_dataset(dataset, config, dim_vars \u001b[39m=\u001b[39m dim_vars)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Split the Data instances in \u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Desktop\\TrafficPrediction_MLG_Project\\src\\scripts\\data_preparation.py:390\u001b[0m, in \u001b[0;36mprepare_pyg_dataset\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39m#Prepare edge_index matrix\u001b[39;00m\n\u001b[0;32m    389\u001b[0m counters_aggregated \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(config[\u001b[39m'\u001b[39m\u001b[39mcounters_nontemporal_aggregated\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 390\u001b[0m edge_index, n_node, _ \u001b[39m=\u001b[39m construct_edge_index(counters_aggregated)\n\u001b[0;32m    391\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mEdge index constructed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    393\u001b[0m \u001b[39m#get dimension values \u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Desktop\\TrafficPrediction_MLG_Project\\src\\scripts\\data_preparation.py:248\u001b[0m, in \u001b[0;36mconstruct_edge_index\u001b[1;34m(counters_aggregated)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_node):\n\u001b[0;32m    247\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_node):\n\u001b[1;32m--> 248\u001b[0m         \u001b[39mif\u001b[39;00m adj_mtx\u001b[39m.\u001b[39;49miloc[i, j] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    249\u001b[0m             edge_index[\u001b[39m0\u001b[39m, num_edges] \u001b[39m=\u001b[39m i\n\u001b[0;32m    250\u001b[0m             edge_index[\u001b[39m1\u001b[39m, num_edges] \u001b[39m=\u001b[39m j\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\core\\indexing.py:1065\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1063\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mlist\u001b[39m(x) \u001b[39mif\u001b[39;00m is_iterator(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m   1064\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[1;32m-> 1065\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_is_scalar_access(key):\n\u001b[0;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m   1067\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\core\\indexing.py:1534\u001b[0m, in \u001b[0;36m_iLocIndexer._is_scalar_access\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1526\u001b[0m \u001b[39mReturns\u001b[39;00m\n\u001b[0;32m   1527\u001b[0m \u001b[39m-------\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m \u001b[39mbool\u001b[39;00m\n\u001b[0;32m   1529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m \u001b[39m# this is a shortcut accessor to both .loc and .iloc\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39m# that provide the equivalent access of .at and .iat\u001b[39;00m\n\u001b[0;32m   1532\u001b[0m \u001b[39m# a) avoid getting things via sections and (to minimize dtype changes)\u001b[39;00m\n\u001b[0;32m   1533\u001b[0m \u001b[39m# b) provide a performant path\u001b[39;00m\n\u001b[1;32m-> 1534\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39;49m(key) \u001b[39m!=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mndim:\n\u001b[0;32m   1535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mall\u001b[39m(is_integer(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m key)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Make runs directory if it does not exist\n",
        "if not os.path.exists(config['RESULTS_DIR'].rsplit('/', 2)[0]):\n",
        "    os.mkdir(config['RESULTS_DIR'].rsplit('/', 2)[0])\n",
        "\n",
        "dataset, dim_vars = data_preparation.prepare_pyg_dataset(config)\n",
        "train_g, val_g, test_g, train_vars, val_vars, test_vars = data_preparation.split_dataset(dataset, config, dim_vars = dim_vars)\n",
        "\n",
        "# Split the Data instances in \n",
        "train_dataloader = DataLoader(train_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
        "val_dataloader = DataLoader(val_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
        "test_dataloader = DataLoader(test_g, batch_size=config['BATCH_SIZE'], shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "Id7THO6YJeLh",
        "outputId": "3ecc2e54-3f8d-4b8f-83b3-6d87e8f81356"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Model initialized\n",
            "Epoch 0: 100%|██████████| 3/3 [00:22<00:00,  7.48s/it]\n",
            "INFO:root:Loss: 787492.438\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Configure and train model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m model_train(train_dataloader, val_dataloader, config, device, \u001b[39mTrue\u001b[39;49;00m, test_dataloader, val_vars)\n",
            "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mmodel_train\u001b[1;34m(train_dataloader, val_dataloader, config, device, save_test_results, test_dataloader, dim_vars)\u001b[0m\n\u001b[0;32m     39\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 41\u001b[0m     train_rmse, train_mae, train_mape, _, _, _ \u001b[39m=\u001b[39m \u001b[39meval\u001b[39;49m(model, device, train_dataloader, \u001b[39m'\u001b[39;49m\u001b[39mTrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     42\u001b[0m     val_rmse, val_mae, val_mape, _, _, _ \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(model, device, val_dataloader, \u001b[39m'\u001b[39m\u001b[39mValid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m config[\u001b[39m\"\u001b[39m\u001b[39muse_tensorboard\u001b[39m\u001b[39m\"\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36meval\u001b[1;34m(model, device, dataloader, type, dim_vars, save_predictions)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 19\u001b[0m         pred \u001b[39m=\u001b[39m model(batch, device)\n\u001b[0;32m     20\u001b[0m     truth \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mview(pred\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn[5], line 75\u001b[0m, in \u001b[0;36mST_GNN.forward\u001b[1;34m(self, data, device)\u001b[0m\n\u001b[0;32m     72\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mFloatTensor(x)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mUSE_GAT\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m---> 75\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgat(x, edge_index)\n\u001b[0;32m     76\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgcn(x, edge_index)\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:241\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    238\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_updater(edge_index, alpha\u001b[39m=\u001b[39malpha, edge_attr\u001b[39m=\u001b[39medge_attr)\n\u001b[0;32m    240\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, alpha\u001b[39m=\u001b[39;49malpha, size\u001b[39m=\u001b[39;49msize)\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcat:\n\u001b[0;32m    244\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels)\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:429\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m decomp_args:\n\u001b[0;32m    427\u001b[0m         kwargs[arg] \u001b[39m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[1;32m--> 429\u001b[0m coll_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__collect__(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__user_args__, edge_index,\n\u001b[0;32m    430\u001b[0m                              size, kwargs)\n\u001b[0;32m    432\u001b[0m msg_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[0;32m    433\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_pre_hooks\u001b[39m.\u001b[39mvalues():\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:301\u001b[0m, in \u001b[0;36mMessagePassing.__collect__\u001b[1;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor):\n\u001b[0;32m    300\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_size__(size, dim, data)\n\u001b[1;32m--> 301\u001b[0m             data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__lift__(data, edge_index, dim)\n\u001b[0;32m    303\u001b[0m         out[arg] \u001b[39m=\u001b[39m data\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
            "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:239\u001b[0m, in \u001b[0;36mMessagePassing.__lift__\u001b[1;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     index \u001b[39m=\u001b[39m edge_index[dim]\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mindex_select(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim, index)\n\u001b[0;32m    240\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mIndexError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    241\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Configure and train model\n",
        "model = model_train(train_dataloader, val_dataloader, config, device, True, test_dataloader, val_vars)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "trafficPrediction39MLG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "adf2a579d8120a92e1286b98590b288d376803eb678f940738ffad32bae242ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
