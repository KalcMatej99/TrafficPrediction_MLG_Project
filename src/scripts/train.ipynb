{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ST-GAT Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "1.13.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import preprocessing as preprocessing\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "print(torch.__version__)\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "#Lets start at src location\n",
    "if os.path.exists(\"./src\"):\n",
    "    os.chdir(\"./src\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "def prepare_data(config):\n",
    "\n",
    "    print(\"Preparing data...\")\n",
    "\n",
    "    if config[\"USE_HOLIDAY_FEATURES\"]:\n",
    "        # holidays for each region\n",
    "        holidays = pd.read_csv(\"../data/holidays.csv\")\n",
    "\n",
    "        # drop holiday region (all values look like NaN - all we have is Austrian regions or very german sounding Slovenian regions)\n",
    "        holidays = holidays.drop(['region'], axis = 1)\n",
    "\n",
    "        # remove NaNs\n",
    "        holidays = holidays.drop_duplicates()\n",
    "\n",
    "        # name holidays properly\n",
    "        holidays.rename(columns = {'date': 'Date'}, inplace = True)\n",
    "\n",
    "        holiday_markers = preprocessing.add_hours_to_holidays(holidays)\n",
    "        holiday_markers[\"Date\"] = pd.to_datetime(holiday_markers['Date']) \n",
    "        slovenian_holiday_markers = holiday_markers[holiday_markers[\"country\"] == \"Slovenia\"]\n",
    "        slovenian_holiday_markers[\"is_holiday\"] = 1\n",
    "\n",
    "\n",
    "    counters_df = pd.DataFrame()\n",
    "    for fname in glob.glob(config[\"counter_files_path\"] + \"*.csv\"):\n",
    "        counter_data = pd.read_csv(fname)\n",
    "        counter_data = preprocessing.fill_gaps(counter_data)\n",
    "        #counter_data = preprocessing.mark_holidays(counter_data, holiday_markers)\n",
    "        counter_data['Date'] = pd.to_datetime(counter_data['Date']) \n",
    "        counter_data.index = counter_data['Date']\n",
    "        counter_data = counter_data.sort_index(ascending=False)\n",
    "        # We don't need to work with all past data.\n",
    "        # Select enough data points to extract N_GRAPHS with F_IN and F_OUT timepoints\n",
    "        \n",
    "        counter_data = counter_data.iloc[0:(config[\"F_IN\"]+config[\"F_OUT\"]+config[\"N_GRAPHS\"]), :]\n",
    "        counter_id = fname.split('/')[-1].split('.csv')[0]\n",
    "\n",
    "        if counters_df.empty:\n",
    "            counters_df = pd.DataFrame(counter_data[config['target_col']])\n",
    "            counters_df.columns = [counter_id]\n",
    "        else:\n",
    "            columns = list(counters_df.columns) + [counter_id]\n",
    "            counters_df = pd.concat([counters_df, counter_data[config['target_col']]], axis=1)\n",
    "            counters_df.columns = columns \n",
    "\n",
    "\n",
    "    #Prepare edge_index matrix\n",
    "    counters_aggregated = pd.read_csv(config['counters_nontemporal_aggregated'])\n",
    "    edge_index, n_node, num_edges = preprocessing.construct_edge_index(counters_aggregated)\n",
    "\n",
    "    #Prepare matrices X [N_GRAPHS, N_NODES, F_IN] and Y [N_GRAPHS, N_NODES, F_OUT] \n",
    "    graphs = []\n",
    "    for i in range(1, config[\"N_GRAPHS\"]+1):\n",
    "        g = Data()\n",
    "        g.__num_nodes__ = n_node\n",
    "        g.edge_index = edge_index\n",
    "        train_test_chunk = counters_df.iloc[(-i-(config['F_IN']+config['F_OUT'])):(-i),:]\n",
    "        \n",
    "        X = train_test_chunk.iloc[:config['F_IN'],:]\n",
    "        Y = train_test_chunk.iloc[config['F_IN']:,:]\n",
    "        \n",
    "        if config[\"USE_HOLIDAY_FEATURES\"]:\n",
    "            X = pd.merge(X, slovenian_holiday_markers[[\"Date\", \"is_holiday\"]], left_on=\"Date\", right_on=\"Date\", how=\"left\")\n",
    "            X['is_holiday'] = X['is_holiday'].fillna(0)\n",
    "            #X['isPaid'] = X['isPaid'].fillna(0)\n",
    "\n",
    "            Y = pd.merge(Y, slovenian_holiday_markers[[\"Date\", \"is_holiday\"]], left_on=\"Date\", right_on=\"Date\", how=\"left\")\n",
    "            Y['is_holiday'] = X['is_holiday'].fillna(0)\n",
    "            #Y['isPaid'] = X['isPaid'].fillna(0)\n",
    "\n",
    "\n",
    "            X.index = X['Date']\n",
    "            X = X.sort_index(ascending=False)\n",
    "            del X['Date']\n",
    "            Y.index = Y['Date']\n",
    "            Y = Y.sort_index(ascending=False)\n",
    "            del Y['Date']\n",
    "\n",
    "        g.x = torch.FloatTensor(X.to_numpy().T)\n",
    "        g.y = torch.FloatTensor(Y.to_numpy().T)\n",
    "        graphs += [g]\n",
    "\n",
    "    splits = (0.6, 0.1, 0.3) # 60% Train, 10% Validation, 30% Test\n",
    "    split_train, split_val, _ = splits\n",
    "    index_train = int(np.floor(config[\"N_GRAPHS\"]*split_train))\n",
    "    index_val = int(index_train + np.floor(config[\"N_GRAPHS\"]*split_val))\n",
    "    train_g = graphs[:index_train]\n",
    "    val_g = graphs[index_train:index_val]\n",
    "    test_g = graphs[index_val:]\n",
    "\n",
    "    print(\"Size of train data:\", len(train_g))\n",
    "    print(\"Size of validation data:\", len(val_g))\n",
    "    print(\"Size of test data:\", len(test_g))\n",
    "\n",
    "    return train_g, val_g, test_g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "class ST_GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-Temporal Graph Attention Network as presented in https://ieeexplore.ieee.org/document/8903252\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, n_nodes, heads=8, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the ST-GAT model\n",
    "        :param in_channels Number of input channels\n",
    "        :param out_channels Number of output channels\n",
    "        :param n_nodes Number of nodes in the graph\n",
    "        :param heads Number of attention heads to use in graph\n",
    "        :param dropout Dropout probability on output of Graph Attention Network\n",
    "        \"\"\"\n",
    "        super(ST_GAT, self).__init__()\n",
    "        self.n_pred = out_channels\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        self.n_preds = 9\n",
    "\n",
    "        self.gat = GATConv(in_channels=in_channels, out_channels=in_channels,\n",
    "                heads=heads, dropout=0, concat=False)\n",
    "\n",
    "        if config[\"USE_LSTM\"]:\n",
    "            self.lstms = []\n",
    "            for layer_index, layer_size in enumerate(config[\"LSTM_LAYER_SIZES\"]):\n",
    "                if layer_index == 0: input_size = self.n_nodes\n",
    "                else: input_size = config[\"LSTM_LAYER_SIZES\"][layer_index - 1]\n",
    "\n",
    "                lstm = torch.nn.LSTM(input_size=input_size, hidden_size=layer_size, num_layers=1)\n",
    "                for name, param in lstm.named_parameters():\n",
    "                    if 'bias' in name:\n",
    "                        torch.nn.init.constant_(param, 0.0)\n",
    "                    elif 'weight' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param)\n",
    "                self.lstms.append(lstm)\n",
    "\n",
    "            # fully-connected neural network\n",
    "            self.linear = torch.nn.Linear(config[\"LSTM_LAYER_SIZES\"][-1], self.n_nodes*self.n_pred)\n",
    "        else:\n",
    "            self.grus = []\n",
    "            for layer_index, layer_size in enumerate(config[\"GRU_LAYER_SIZES\"]):\n",
    "                if layer_index == 0: input_size = self.n_nodes\n",
    "                else: input_size = config[\"GRU_LAYER_SIZES\"][layer_index - 1]\n",
    "\n",
    "                lstm = torch.nn.GRU(input_size=input_size, hidden_size=layer_size, num_layers=1)\n",
    "                self.grus.append(lstm)\n",
    "\n",
    "            # fully-connected neural network\n",
    "            self.linear = torch.nn.Linear(config[\"GRU_LAYER_SIZES\"][-1], self.n_nodes*self.n_pred)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, data, device):\n",
    "        \"\"\"\n",
    "        Forward pass of the ST-GAT model\n",
    "        :param data Data to make a pass on\n",
    "        :param device Device to operate on\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # apply dropout\n",
    "        if device == 'cpu':\n",
    "            x = torch.FloatTensor(x)\n",
    "        else:\n",
    "            x = torch.cuda.FloatTensor(x)\n",
    "\n",
    "        x = self.gat(x, edge_index)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "\n",
    "            # RNN: 2 LSTM\n",
    "        batch_size = data.num_graphs\n",
    "        n_node = int(data.num_nodes/batch_size)\n",
    "        x = torch.reshape(x, (batch_size, n_node, data.num_features))\n",
    "        x = torch.movedim(x, 2, 0)\n",
    "        if config[\"USE_LSTM\"]:\n",
    "            for lstm in self.lstms:\n",
    "                x, _ = lstm(x)\n",
    "        else:\n",
    "            for gru in self.grus:\n",
    "                x, _ = gru(x)\n",
    "\n",
    "\n",
    "        x = torch.squeeze(x[-1, :, :])\n",
    "        x = self.linear(x)\n",
    "\n",
    "        s = x.shape\n",
    "        x = torch.reshape(x, (s[0], self.n_nodes, self.n_pred))\n",
    "        x = torch.reshape(x, (s[0]*self.n_nodes, self.n_pred))\n",
    "        return x\n",
    "    \n",
    "\n",
    "class ST_GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_nodes, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the ST-GAT model\n",
    "        :param in_channels Number of input channels\n",
    "        :param out_channels Number of output channels\n",
    "        :param n_nodes Number of nodes in the graph\n",
    "        :param heads Number of attention heads to use in graph\n",
    "        :param dropout Dropout probability on output of Graph Attention Network\n",
    "        \"\"\"\n",
    "        super(ST_GCN, self).__init__()\n",
    "        self.n_pred = out_channels\n",
    "        self.dropout = dropout\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        self.n_preds = 9\n",
    "\n",
    "        self.gcn = GCNConv(in_channels=in_channels, out_channels=in_channels, dropout=0, concat=False)\n",
    "\n",
    "        if config[\"USE_LSTM\"]:\n",
    "            self.lstms = []\n",
    "            for layer_index, layer_size in enumerate(config[\"LSTM_LAYER_SIZES\"]):\n",
    "                if layer_index == 0: input_size = self.n_nodes\n",
    "                else: input_size = config[\"LSTM_LAYER_SIZES\"][layer_index - 1]\n",
    "\n",
    "                lstm = torch.nn.LSTM(input_size=input_size, hidden_size=layer_size, num_layers=1)\n",
    "                for name, param in lstm.named_parameters():\n",
    "                    if 'bias' in name:\n",
    "                        torch.nn.init.constant_(param, 0.0)\n",
    "                    elif 'weight' in name:\n",
    "                        torch.nn.init.xavier_uniform_(param)\n",
    "                self.lstms.append(lstm)\n",
    "\n",
    "            # fully-connected neural network\n",
    "            self.linear = torch.nn.Linear(config[\"LSTM_LAYER_SIZES\"][-1], self.n_nodes*self.n_pred)\n",
    "        else:\n",
    "            self.grus = []\n",
    "            for layer_index, layer_size in enumerate(config[\"GRU_LAYER_SIZES\"]):\n",
    "                if layer_index == 0: input_size = self.n_nodes\n",
    "                else: input_size = config[\"GRU_LAYER_SIZES\"][layer_index - 1]\n",
    "\n",
    "                lstm = torch.nn.GRU(input_size=input_size, hidden_size=layer_size, num_layers=1)\n",
    "                self.grus.append(lstm)\n",
    "\n",
    "            # fully-connected neural network\n",
    "            self.linear = torch.nn.Linear(config[\"GRU_LAYER_SIZES\"][-1], self.n_nodes*self.n_pred)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, data, device):\n",
    "        \"\"\"\n",
    "        Forward pass of the ST-GAT model\n",
    "        :param data Data to make a pass on\n",
    "        :param device Device to operate on\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # apply dropout\n",
    "        if device == 'cpu':\n",
    "            x = torch.FloatTensor(x)\n",
    "        else:\n",
    "            x = torch.cuda.FloatTensor(x)\n",
    "\n",
    "        x = self.gcn(x, edge_index)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        # RNN: 2 LSTM\n",
    "        batch_size = data.num_graphs\n",
    "        n_node = int(data.num_nodes/batch_size)\n",
    "        x = torch.reshape(x, (batch_size, n_node, data.num_features))\n",
    "        x = torch.movedim(x, 2, 0)\n",
    "        if config[\"USE_LSTM\"]:\n",
    "            for lstm in self.lstms:\n",
    "                x, _ = lstm(x)\n",
    "        else:\n",
    "            for gru in self.grus:\n",
    "                x, _ = gru(x)\n",
    "\n",
    "\n",
    "        x = torch.squeeze(x[-1, :, :])\n",
    "        x = self.linear(x)\n",
    "\n",
    "        s = x.shape\n",
    "        x = torch.reshape(x, (s[0], self.n_nodes, self.n_pred))\n",
    "        x = torch.reshape(x, (s[0]*self.n_nodes, self.n_pred))\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def model_train(train_dataloader, val_dataloader, config, device, save_test_results = False, test_dataloader = None):\n",
    "    \"\"\"\n",
    "    Train the ST-GAT model. Evaluate on validation dataset as you go.\n",
    "    :param train_dataloader Data loader of training dataset\n",
    "    :param val_dataloader Dataloader of val dataset\n",
    "    :param config configuration to use\n",
    "    :param device Device to evaluate on\n",
    "    \"\"\"\n",
    "\n",
    "    # Make the model. Each datapoint in the graph is 228x12: N x F (N = # nodes, F = time window)\n",
    "    if config[\"USE_GAT\"]:\n",
    "        model = ST_GAT(in_channels=config['F_IN'], out_channels=config['F_OUT'], n_nodes=config['N_NODE'], dropout=config['DROPOUT'])\n",
    "    else:\n",
    "        model = ST_GCN(in_channels=config['F_IN'], out_channels=config['F_OUT'], n_nodes=config['N_NODE'], dropout=config['DROPOUT'])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['INITIAL_LR'], weight_decay=config['WEIGHT_DECAY'])\n",
    "    loss_fn = torch.nn.MSELoss\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # For every epoch, train the model on training dataset. Evaluate model on validation dataset\n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        loss = train(model, device, train_dataloader, optimizer, loss_fn, epoch)\n",
    "        print(f\"Loss: {loss:.3f}\")\n",
    "        if epoch % 5 == 0:\n",
    "            train_mae, train_rmse, train_mape, _, _ = eval(model, device, train_dataloader, 'Train')\n",
    "            val_mae, val_rmse, val_mape, _, _ = eval(model, device, val_dataloader, 'Valid')\n",
    "            if config[\"use_tensorboard\"]:\n",
    "                writer.add_scalar(f\"MAE/train\", train_mae, epoch)\n",
    "                writer.add_scalar(f\"RMSE/train\", train_rmse, epoch)\n",
    "                writer.add_scalar(f\"MAPE/train\", train_mape, epoch)\n",
    "                writer.add_scalar(f\"MAE/val\", val_mae, epoch)\n",
    "                writer.add_scalar(f\"RMSE/val\", val_rmse, epoch)\n",
    "                writer.add_scalar(f\"MAPE/val\", val_mape, epoch)\n",
    "\n",
    "    if config[\"use_tensorboard\"]:\n",
    "        writer.flush()\n",
    "    # Save the model\n",
    "    timestr = time.strftime(\"%m-%d-%H%M%S\")\n",
    "    os.mkdir(os.path.join(config[\"CHECKPOINT_DIR\"], f\"run_{timestr}\"))\n",
    "    torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "            }, os.path.join(config[\"CHECKPOINT_DIR\"], f\"run_{timestr}/model.pt\"))\n",
    "    \n",
    "    with open(os.path.join(config[\"CHECKPOINT_DIR\"], f\"run_{timestr}/config.json\"), \"w\") as fp:\n",
    "        json.dump(config, fp)\n",
    "\n",
    "    if save_test_results:\n",
    "        test_mae, test_rmse, test_mape, y_pred, y_truth = eval(model, device, test_dataloader, 'Test')\n",
    "        results = {'MAE': test_mae.item(),\n",
    "                    'RMSE': test_rmse.item(),\n",
    "                    'MAPE': test_mape.item()}\n",
    "        with open(os.path.join(config[\"CHECKPOINT_DIR\"], f\"run_{timestr}/results.json\"), \"w\") as fp:\n",
    "            json.dump(results, fp)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "def un_z_score(x_normed, mean, std):\n",
    "    return x_normed * std  + mean\n",
    "def MAPE(v, v_):\n",
    "    return torch.mean(torch.abs((v_ - v)) /(v + 1e-15) * 100)\n",
    "def RMSE(v, v_):\n",
    "    return torch.sqrt(torch.mean((v_ - v) ** 2))\n",
    "def MAE(v, v_):\n",
    "    return torch.mean(torch.abs(v_ - v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, device, dataloader, type=''):\n",
    "    \"\"\"\n",
    "    Evaluation function to evaluate model on data\n",
    "    :param model Model to evaluate\n",
    "    :param device Device to evaluate on\n",
    "    :param dataloader Data loader\n",
    "    :param type Name of evaluation type, e.g. Train/Val/Test\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    mae = 0\n",
    "    rmse = 0\n",
    "    mape = 0\n",
    "    n = 0\n",
    "\n",
    "    # Evaluate model on all data\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch, device)\n",
    "            truth = batch.y.view(pred.shape)\n",
    "            if i == 0:\n",
    "                y_pred = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
    "                y_truth = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
    "            #truth = un_z_score(truth, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
    "            #pred = un_z_score(pred, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
    "            y_pred[i, :pred.shape[0], :] = pred\n",
    "            y_truth[i, :pred.shape[0], :] = truth\n",
    "            rmse += RMSE(truth, pred)\n",
    "            mae += MAE(truth, pred)\n",
    "            mape += MAPE(truth, pred)\n",
    "            n += 1\n",
    "    rmse, mae, mape = rmse / n, mae / n, mape / n\n",
    "\n",
    "    print(f'{type}, MAE: {mae}, RMSE: {rmse}, MAPE: {mape}')\n",
    "\n",
    "    #get the average score for each metric in each batch\n",
    "    return rmse, mae, mape, y_pred, y_truth\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "\n",
    "def train(model, device, dataloader, optimizer, loss_fn, epoch):\n",
    "    \"\"\"\n",
    "    Evaluation function to evaluate model on data\n",
    "    :param model Model to evaluate\n",
    "    :param device Device to evaluate on\n",
    "    :param dataloader Data loader\n",
    "    :param optimizer Optimizer to use\n",
    "    :param loss_fn Loss function\n",
    "    :param epoch Current epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    model.train()\n",
    "    for _, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = torch.squeeze(model(batch, device))\n",
    "        loss = loss_fn()(y_pred.float(), torch.squeeze(batch.y).float())\n",
    "        if config[\"use_tensorboard\"]:\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # multiplicative decay\n",
    "        scheduler.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/counters_non_temporal_aggregated_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(config[\u001b[39m\"\u001b[39m\u001b[39mCHECKPOINT_DIR\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[0;32m     32\u001b[0m     os\u001b[39m.\u001b[39mmkdir(config[\u001b[39m\"\u001b[39m\u001b[39mCHECKPOINT_DIR\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m---> 34\u001b[0m train_g, val_g, test_g \u001b[39m=\u001b[39m prepare_data(config)\n\u001b[0;32m     35\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_g, batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mBATCH_SIZE\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m val_dataloader \u001b[39m=\u001b[39m DataLoader(val_g, batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mBATCH_SIZE\u001b[39m\u001b[39m'\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     47\u001b[0m         counters_df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m columns \n\u001b[0;32m     50\u001b[0m \u001b[39m#Prepare edge_index matrix\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m counters_aggregated \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(config[\u001b[39m'\u001b[39;49m\u001b[39mcounters_nontemporal_aggregated\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     52\u001b[0m edge_index, n_node, num_edges \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mconstruct_edge_index(counters_aggregated)\n\u001b[0;32m     54\u001b[0m \u001b[39m#Prepare matrices X [N_GRAPHS, N_NODES, F_IN] and Y [N_GRAPHS, N_NODES, F_OUT] \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\markoi\\Anaconda3\\envs\\dars-marko\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/counters_non_temporal_aggregated_data.csv'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Constant config to use throughout\n",
    "config = {\n",
    "    'BATCH_SIZE': 50,\n",
    "    'EPOCHS': 60,\n",
    "    'WEIGHT_DECAY': 5e-5,\n",
    "    'INITIAL_LR': 1e-1,\n",
    "    'CHECKPOINT_DIR': '../runs',\n",
    "    'DROPOUT': 0.2,\n",
    "    \"counter_files_path\"                : \"../data/counters_temporal_data_2023-03-03T09-24-06/\",\n",
    "    \"counters_nontemporal_aggregated\"   : \"../data/counters_non_temporal_aggregated_data.csv\",\n",
    "    \"USE_HOLIDAY_FEATURES\"              : False, # CURRENTLY BROKEN, DO NO SET TO True!!\n",
    "    \"N_GRAPHS\"                          : 30*24,\n",
    "    \"F_IN\"                              : 7*24,\n",
    "    \"F_OUT\"                             : 7*24,\n",
    "    \"N_NODE\"                            : 165,\n",
    "    \"target_col\"                        : \"Sum\",\n",
    "    \"use_tensorboard\"                   : False,\n",
    "    \"USE_GAT\"                           : True, # if True use GAT, else use GCN\n",
    "    \"USE_LSTM\"                          : False, # if True use LSTM, else use GRU\n",
    "    \"LSTM_LAYER_SIZES\"                  : [128, 32],  \n",
    "    \"GRU_LAYER_SIZES\"                  : [128, 32],    \n",
    "}\n",
    "\n",
    "\n",
    "# Make a tensorboard writer\n",
    "if config[\"use_tensorboard\"]:\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "if not os.path.exists(config[\"CHECKPOINT_DIR\"]):\n",
    "    os.mkdir(config[\"CHECKPOINT_DIR\"])\n",
    "\n",
    "train_g, val_g, test_g = prepare_data(config)\n",
    "train_dataloader = DataLoader(train_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "val_dataloader = DataLoader(val_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "test_dataloader = DataLoader(test_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "# Get gpu if you can\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Configure and train model\n",
    "model = model_train(train_dataloader, val_dataloader, config, device, True, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trafficPrediction39MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adf2a579d8120a92e1286b98590b288d376803eb678f940738ffad32bae242ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
