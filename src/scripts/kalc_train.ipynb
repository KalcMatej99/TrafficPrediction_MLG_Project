{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5imzg7fEJeLM"
      },
      "source": [
        "# Traffic prediction modeling with GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3urCm8nvnFrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd8e55b-6750-4847-fdd0-34241331a101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "PyTorch has version 1.13.1+cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.2.0.tar.gz (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 KB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.2.0-py3-none-any.whl size=773302 sha256=4932b27737661ed5352f4ee7f36199799db37c8b96612917af6d2598e4185bbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/b2/8c/9b4bb72a4384eabd1ffeab2b7ead692c9165e35711f8a9dc72\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.5-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.26.15)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.22.4)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.4.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from ogb) (1.13.1+cu116)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (63.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated>=0.2.0->ogb) (2.27.1)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.12.7)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=963d0fcbc4e47952346f509c0982f21efdc9f270e7cc321638140b1d23915421\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/bb/0d/2d02ec45f29c48d6192476bfb59c5a0e64b605e7212374dd15\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.5 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "# Only colab stuff\n",
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  import torch\n",
        "  import os\n",
        "  print(\"PyTorch has version {}\".format(torch.__version__))\n",
        "  if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "    !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
        "    !pip install torch-geometric\n",
        "    !pip install ogb\n",
        "\n",
        "    os.chdir(\"/content/gdrive/MyDrive/MLG_cloned_repo/src/scripts\")\n",
        "except:\n",
        "  print(\"Not in colab!\")\n",
        "  os.chdir(\"./src/scripts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gefmlTmrM8TH",
        "outputId": "5d389521-7c8e-4465-e1e9-e8a813e6f797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/MLG_cloned_repo/src/scripts\n"
          ]
        }
      ],
      "source": [
        "#You should be in src/scripts!!!\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8EIVXxkJeLQ"
      },
      "source": [
        "### Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE_usvT1JeLR",
        "outputId": "908dcd00-4d0e-43a5-86bc-47017f759a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n",
            "Using cuda\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "#Custom scripts\n",
        "import modeling_utils as modeling_utils \n",
        "import data_preparation as data_preparation\n",
        "\n",
        "#Pytorch and PyG\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR \n",
        "from torch_geometric.nn import GATConv, GCNConv, GATv2Conv\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "print(torch.__version__)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using {device}\")\n",
        "\n",
        "#Lets start at src location\n",
        "if os.path.exists(\"./src\"):\n",
        "  os.chdir(\"./src\")\n",
        "elif 'scripts' in os.getcwd():\n",
        "  os.chdir(\"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E29uodRcJeLU"
      },
      "outputs": [],
      "source": [
        "#%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PugM8CEeJeLV"
      },
      "source": [
        "### Constants and setting-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8Wtx_uTJeLW"
      },
      "outputs": [],
      "source": [
        "# Constant config to use throughout\n",
        "config = {\n",
        "    'TRAIN_TEST_PROPORTION'             : (0.7, 0.1, 0.2),\n",
        "    'BATCH_SIZE'                        : 128,\n",
        "    'EPOCHS'                            : 200,\n",
        "    'WEIGHT_DECAY'                      : 0,\n",
        "    'INITIAL_LR'                        : 0.1,\n",
        "    'LR_DECAY'                          : 0.5,\n",
        "    'LR_DECAY_STEP'                     : 50,\n",
        "    'DROPOUT'                           : 0.0,\n",
        "    'ATTENTION_HEADS'                   : 8,\n",
        "    'RESULTS_DIR'                       : './runs/'+time.strftime(\"%m-%dT%H-%M-%S\")+'/',\n",
        "    'data_with_already_filled_gaps'     : True,\n",
        "    'counter_files_path'                : '../data/counters_interpolated_gaps/',                         # '../data/counters_temporal_data_2023-03-03T09-24-06/'\n",
        "    'counters_nontemporal_aggregated'   : '../data/counters_non_temporal_aggregated_data.csv',\n",
        "    'holidays_path'                     : '../data/holidays.csv',\n",
        "    'USE_YEAR_PERIODIC_DATA'            : False,\n",
        "    'USE_HOLIDAY_FEATURES'              : False,\n",
        "    'USE_WEEKDAY_FEATURES'              : True,\n",
        "    'USE_MONTH_FEATURES'                : False,\n",
        "    'N_GRAPHS'                          : 380*24,\n",
        "    'F_IN'                              : 7*24,\n",
        "    'F_OUT'                             : 7*24,\n",
        "    'N_NODE'                            : 165,\n",
        "    'target_col'                        : 'Fast',\n",
        "    'use_tensorboard'                   : True,\n",
        "    'USE_GAT'                           : True, # if True use GAT, else use GCN\n",
        "    'USE_LSTM'                          : True, # if True use LSTM, else use GRU\n",
        "    'LSTM_LAYER_SIZES'                  : [500, 500],  \n",
        "    'GRU_LAYER_SIZES'                   : [500, 500],  \n",
        "    'LINEAR_HIDDEN_SIZE'                : 600,     \n",
        "    'USE_EARLY_STOPPING'                : True,\n",
        "    \"MIN_ITERATIONS_EARLY_STOPPING\"     : 50,\n",
        "    \"EARLY_STOPPING_TOLERANCE\"          : 15,\n",
        "    \"LOG_BASELINE\"                      : True, # if true outputs average rmse on computed on each batch,\n",
        "    \"DATA_DATE_SPLIT\"                   : '05/07/22 00:00:00',\n",
        "    \"SCALE_DATA\"                        : False,\n",
        "    \"USE_ONEHOT_FEATURES\"               : False\n",
        "}\n",
        "\n",
        "# Set logging level\n",
        "logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "# Make a tensorboard writer\n",
        "if config[\"use_tensorboard\"]:\n",
        "    writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTedXzi9JeLY"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ69dl_7JeLZ"
      },
      "outputs": [],
      "source": [
        "class ST_GNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Spatio-Temporal Graph Neural Network which has options of using:\n",
        "    1) Normal neighbor aggregation OR attention mechanism\n",
        "    2) GRU temporal layer or LSTM\n",
        "    \"\"\" \n",
        "    def __init__(self, device, in_channels, out_channels, n_nodes, heads=None, dropout=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the ST-GNN model\n",
        "        :param in_channels Number of input channels\n",
        "        :param out_channels Number of output channels\n",
        "        :param n_nodes Number of nodes in the graph\n",
        "        :param heads Number of attention heads to use in graph\n",
        "        :param dropout Dropout probability on output of Graph Attention Network\n",
        "        \"\"\"\n",
        "        # Set up params\n",
        "        super(ST_GNN, self).__init__()\n",
        "        self.device = device\n",
        "        self.n_pred = out_channels\n",
        "        self.dropout = dropout\n",
        "        self.n_nodes = n_nodes\n",
        "        #self.n_preds = 9 TODO is this needed?\n",
        "        \n",
        "        # Init spatial part\n",
        "        if config['USE_GAT']:\n",
        "            self.heads = heads\n",
        "            self.gat = GATv2Conv(in_channels=in_channels, out_channels=in_channels,\n",
        "                    heads=heads, dropout=0, concat=False)\n",
        "        else:\n",
        "            self.gcn = GCNConv(in_channels=in_channels, out_channels=in_channels, dropout=0, concat=False)\n",
        "\n",
        "        # Init temporal part\n",
        "        if config['USE_LSTM']:\n",
        "            self.lstms = []\n",
        "            for layer_index, layer_size in enumerate(config[\"LSTM_LAYER_SIZES\"]):\n",
        "                if layer_index == 0: input_size = self.n_nodes\n",
        "                else: input_size = config[\"LSTM_LAYER_SIZES\"][layer_index - 1]\n",
        "\n",
        "                lstm = torch.nn.LSTM(input_size=input_size, hidden_size=layer_size, num_layers=1, device = self.device)\n",
        "                for name, param in lstm.named_parameters():\n",
        "                    if 'bias' in name:\n",
        "                        torch.nn.init.constant_(param, 0.0)\n",
        "                    elif 'weight' in name:\n",
        "                        torch.nn.init.xavier_uniform_(param)\n",
        "                self.lstms.append(lstm)\n",
        "\n",
        "            # fully-connected neural network\n",
        "            self.linear1 = torch.nn.Linear(config[\"LSTM_LAYER_SIZES\"][-1], config[\"LINEAR_HIDDEN_SIZE\"])\n",
        "            self.linear2 = torch.nn.Linear(config[\"LINEAR_HIDDEN_SIZE\"], self.n_nodes*self.n_pred)\n",
        "        else:\n",
        "            self.grus = []\n",
        "            for layer_index, layer_size in enumerate(config[\"GRU_LAYER_SIZES\"]):\n",
        "                if layer_index == 0: input_size = self.n_nodes\n",
        "                else: input_size = config[\"GRU_LAYER_SIZES\"][layer_index - 1]\n",
        "\n",
        "                gru = torch.nn.GRU(input_size=input_size, hidden_size=layer_size, num_layers=1, device = self.device)\n",
        "                self.grus.append(gru)\n",
        "\n",
        "            # fully-connected neural network\n",
        "            self.linear1 = torch.nn.Linear(config[\"GRU_LAYER_SIZES\"][-1], config[\"LINEAR_HIDDEN_SIZE\"])\n",
        "            self.linear2 = torch.nn.Linear(config[\"LINEAR_HIDDEN_SIZE\"], self.n_nodes*self.n_pred)\n",
        "\n",
        "        torch.nn.init.xavier_uniform_(self.linear1.weight)\n",
        "        torch.nn.init.xavier_uniform_(self.linear2.weight)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, data, device):\n",
        "        \"\"\"\n",
        "        Forward pass of the ST-GNN model\n",
        "        :param data Data to make a pass on\n",
        "        :param device Device to operate on\n",
        "        \"\"\"\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        if self.device == 'cpu':\n",
        "            x = torch.FloatTensor(x)\n",
        "        else:\n",
        "            x = torch.cuda.FloatTensor(x)\n",
        "\n",
        "        if config['USE_GAT']:\n",
        "            x = self.gat(x, edge_index)\n",
        "        else:\n",
        "            x = self.gcn(x, edge_index)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "\n",
        "        batch_size = data.num_graphs\n",
        "        n_node = int(data.num_nodes/batch_size)\n",
        "        x = torch.reshape(x, (batch_size, n_node, data.num_features))\n",
        "        x = torch.movedim(x, 2, 0)\n",
        "        if config[\"USE_LSTM\"]:\n",
        "            for lstm in self.lstms:\n",
        "                x, _ = lstm(x)\n",
        "        else:\n",
        "            for gru in self.grus:\n",
        "                x, _ = gru(x)\n",
        "\n",
        "\n",
        "        x = torch.squeeze(x[-1, :, :])\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        s = x.shape\n",
        "        x = torch.reshape(x, (s[0], self.n_nodes, self.n_pred))\n",
        "        x = torch.reshape(x, (s[0]*self.n_nodes, self.n_pred))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XikFEW33JeLb"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval(model, device, dataloader, type='', dim_vars=None, save_predictions=False):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    mae = 0\n",
        "    rmse = 0\n",
        "    baseline_rmse = 0\n",
        "    mape = 0\n",
        "    n = 0\n",
        "\n",
        "    # Evaluate model on all data\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        batch = batch.to(device)\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch, device)\n",
        "            truth = batch.y.view(pred.shape)\n",
        "            if i == 0:\n",
        "                y_pred = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
        "                y_truth = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
        "            #truth = un_z_score(truth, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
        "            #pred = un_z_score(pred, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
        "            \n",
        "            # reshape predictions\n",
        "            y_pred[i, :pred.shape[0], :] = pred\n",
        "            y_truth[i, :pred.shape[0], :] = truth\n",
        "\n",
        "            # save y_prediction & true values for later analysis\n",
        "            if save_predictions:\n",
        "                modeling_utils.save_all_predictions(y_pred, y_truth, dim_vars, config['RESULTS_DIR'])\n",
        "\n",
        "            # calculate batch average (take info only from x and take mean)\n",
        "            pred_avg = torch.mean(batch.x[:,:config['F_IN']], axis=1, keepdim=True).repeat(1,config['F_OUT'])\n",
        "\n",
        "            # calculate loss\n",
        "            rmse += modeling_utils.RMSE(truth, pred)\n",
        "            baseline_rmse += modeling_utils.RMSE(truth, pred_avg)\n",
        "            mae += modeling_utils.MAE(truth, pred)\n",
        "            mape += modeling_utils.MAPE(truth, pred)\n",
        "\n",
        "            n += 1\n",
        "    rmse, mae, mape, baseline_rmse = rmse / n, mae / n, mape / n, baseline_rmse / n\n",
        "\n",
        "    logging.info(f'{type}, MAE: {round(int(mae),2)}, RMSE: {round(int(rmse),2)}, MAPE: {round(int(mape),2)}')\n",
        "\n",
        "    #get the average score for each metric in each batch\n",
        "    return rmse, mae, mape, baseline_rmse, y_pred, y_truth\n",
        "\n",
        "\n",
        "def epoch_train(model, device, dataloader, optimizer, loss_fn, epoch):    \n",
        "    scheduler = StepLR(optimizer, step_size = config['LR_DECAY_STEP'], gamma = config['LR_DECAY'])\n",
        "\n",
        "    model.train()\n",
        "    for _, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = torch.squeeze(model(batch, device))\n",
        "        loss = loss_fn()(y_pred.float(), torch.squeeze(batch.y).float())\n",
        "        if config[\"use_tensorboard\"]:\n",
        "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGNsWYRnJeLd"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F3SQDz9JeLd"
      },
      "outputs": [],
      "source": [
        "def model_train(train_dataloader, val_dataloader, config, device, save_test_results = False, test_dataloader = None, dim_vars = None):\n",
        "    \"\"\"\n",
        "    Train the ST-GAT model. Evaluate on validation dataset as you go.\n",
        "    :param train_dataloader Data loader of training dataset\n",
        "    :param val_dataloader Dataloader of val dataset\n",
        "    :param config configuration to use\n",
        "    :param device Device to evaluate on\n",
        "    \"\"\"\n",
        "\n",
        "    # Make the model. Each datapoint in the graph is 228x12: N x F (N = # nodes, F = time window)\n",
        "    in_channels=config['F_IN']\n",
        "    if config[\"USE_YEAR_PERIODIC_DATA\"]:\n",
        "        in_channels += 2 * 2 + 1\n",
        "    if config[\"USE_HOLIDAY_FEATURES\"]: \n",
        "        in_channels += 7 * data_preparation.number_of_countries_in_holiday_dataset(config)\n",
        "    if config[\"USE_WEEKDAY_FEATURES\"]:\n",
        "        in_channels += 1\n",
        "    if config[\"USE_MONTH_FEATURES\"]:\n",
        "        in_channels += 1\n",
        "    if config[\"USE_ONEHOT_FEATURES\"]:\n",
        "        in_channels += config[\"N_NODE\"]\n",
        "    \n",
        "    model = ST_GNN(\n",
        "        device = device,\n",
        "        in_channels=in_channels, \n",
        "        out_channels=config['F_OUT'], \n",
        "        n_nodes=config['N_NODE'], \n",
        "        heads=config['ATTENTION_HEADS'], \n",
        "        dropout=config['DROPOUT']\n",
        "    )\n",
        "    \n",
        "    logging.info(\"Model initialized\")\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['INITIAL_LR']) #weight_decay=config['WEIGHT_DECAY'])\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=config['INITIAL_LR'], weight_decay=config['WEIGHT_DECAY'])\n",
        "    loss_fn = torch.nn.MSELoss\n",
        "    model.to(device)\n",
        "\n",
        "    # Early stopping variables\n",
        "    n_iteration_since_loss_improvment = 0\n",
        "    best_train_loss = 999999999999999999\n",
        "\n",
        "    # For every epoch, train the model on training dataset. Evaluate model on validation dataset\n",
        "    for epoch in range(config['EPOCHS']):\n",
        "        loss = epoch_train(model, device, train_dataloader, optimizer, loss_fn, epoch)\n",
        "        logging.info(f\"Loss: {loss:.3f}\")\n",
        "        if epoch % 5 == 0:\n",
        "            train_rmse, train_mae, train_mape, _, _, _ = eval(model, device, train_dataloader, 'Train')\n",
        "            val_rmse, val_mae, val_mape, _, _, _ = eval(model, device, val_dataloader, 'Valid')\n",
        "            _, _, _, _, _, _ = eval(model, device, test_dataloader, 'Test')\n",
        "            if config[\"use_tensorboard\"]:\n",
        "                writer.add_scalar(f\"MAE/train\", train_mae, epoch)\n",
        "                writer.add_scalar(f\"RMSE/train\", train_rmse, epoch)\n",
        "                writer.add_scalar(f\"MAPE/train\", train_mape, epoch)\n",
        "                writer.add_scalar(f\"MAE/val\", val_mae, epoch)\n",
        "                writer.add_scalar(f\"RMSE/val\", val_rmse, epoch)\n",
        "                writer.add_scalar(f\"MAPE/val\", val_mape, epoch)\n",
        "        \n",
        "        if config[\"USE_EARLY_STOPPING\"]:\n",
        "          if loss < best_train_loss:\n",
        "            best_train_loss = loss\n",
        "            n_iteration_since_loss_improvment = 0\n",
        "          else: n_iteration_since_loss_improvment += 1\n",
        "\n",
        "          if epoch >= config[\"MIN_ITERATIONS_EARLY_STOPPING\"] and \\\n",
        "                n_iteration_since_loss_improvment >= config[\"EARLY_STOPPING_TOLERANCE\"]:\n",
        "            break\n",
        "    logging.info(\"All epochs done, finished training\")\n",
        "\n",
        "    if config[\"use_tensorboard\"]:\n",
        "        writer.flush()\n",
        "    # Save the model\n",
        "    os.mkdir(config[\"RESULTS_DIR\"])\n",
        "    torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"loss\": loss},\n",
        "            os.path.join(config[\"RESULTS_DIR\"], \"model.pt\")\n",
        "    )\n",
        "    \n",
        "    with open(os.path.join(config[\"RESULTS_DIR\"], \"config.json\"), \"w\") as fp:\n",
        "        json.dump(config, fp)\n",
        "\n",
        "    if save_test_results:\n",
        "        test_rmse, test_mae, test_mape, baseline_rmse, _, _ = eval(model, device, test_dataloader, 'Test', dim_vars, save_predictions=True)\n",
        "        logging.info(f\"Test RMSE:{test_rmse}\")\n",
        "        if config['LOG_BASELINE']:\n",
        "          logging.info(f\"Test BASELINE RMSE:{baseline_rmse}\")\n",
        "        results = {'MAE': test_mae.item(),\n",
        "                    'RMSE': test_rmse.item(),\n",
        "                    'MAPE': test_mape.item(),\n",
        "                   'BASELINE_RMSE': baseline_rmse}\n",
        "        with open(os.path.join(config[\"RESULTS_DIR\"], \"results.json\"), \"w\") as fp:\n",
        "            json.dump(results, fp)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP3A3akKJeLf"
      },
      "source": [
        "### Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYwcGOI2JeLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7524ed31-b480-405e-fd8c-19812873b5b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "#import importlib\n",
        "#importlib.reload(data_preparation)\n",
        "#importlib.reload(modeling_utils)\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIYqn84HJeLg",
        "outputId": "daf8080e-fa78-42c8-b0a8-0c40beddc11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Preparing data...\n",
            "INFO:root:Historical counter data successfully read\n",
            "INFO:root:Edge index constructed\n",
            "INFO:root:Final dataset constructed\n",
            "INFO:root:Dataset splitted to train,val,test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train data: 6384\n",
            "Size of validation data: 912\n",
            "Size of test data: 1824\n"
          ]
        }
      ],
      "source": [
        "# Make runs directory if it does not exist\n",
        "if not os.path.exists(config['RESULTS_DIR'].rsplit('/', 2)[0]):\n",
        "    os.mkdir(config['RESULTS_DIR'].rsplit('/', 2)[0])\n",
        "\n",
        "dataset, dim_vars = data_preparation.prepare_pyg_dataset(config)\n",
        "train_g, val_g, test_g, train_vars, val_vars, test_vars = data_preparation.split_dataset(dataset, config, dim_vars = dim_vars)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the Data instances in \n",
        "train_dataloader = DataLoader(train_g, batch_size=config['BATCH_SIZE'], shuffle=False, drop_last = True)\n",
        "val_dataloader = DataLoader(val_g, batch_size=config['BATCH_SIZE'], shuffle=False, drop_last = True)\n",
        "test_dataloader = DataLoader(test_g, batch_size=config['BATCH_SIZE'], shuffle=False, drop_last = True)"
      ],
      "metadata": {
        "id": "4BEGfvybkil-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id7THO6YJeLh",
        "outputId": "3089228e-a905-4c90-de5e-95e8288dad21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Model initialized\n",
            "Epoch 0: 100%|██████████| 49/49 [00:11<00:00,  4.18it/s]\n",
            "INFO:root:Loss: 234993.406\n",
            "INFO:root:Train, MAE: 406, RMSE: 567, MAPE: 160385572165124096\n",
            "INFO:root:Valid, MAE: 315, RMSE: 447, MAPE: 41769544796078080\n",
            "INFO:root:Test, MAE: 359, RMSE: 506, MAPE: 220252674627469312\n",
            "Epoch 1: 100%|██████████| 49/49 [00:11<00:00,  4.15it/s]\n",
            "INFO:root:Loss: 130093.719\n",
            "Epoch 2: 100%|██████████| 49/49 [00:11<00:00,  4.25it/s]\n",
            "INFO:root:Loss: 129379.125\n",
            "Epoch 3: 100%|██████████| 49/49 [00:11<00:00,  4.33it/s]\n",
            "INFO:root:Loss: 122129.359\n",
            "Epoch 4: 100%|██████████| 49/49 [00:11<00:00,  4.34it/s]\n",
            "INFO:root:Loss: 123710.391\n",
            "Epoch 5: 100%|██████████| 49/49 [00:11<00:00,  4.32it/s]\n",
            "INFO:root:Loss: 98174.359\n",
            "INFO:root:Train, MAE: 280, RMSE: 413, MAPE: 162803862091071488\n",
            "INFO:root:Valid, MAE: 206, RMSE: 317, MAPE: 170371749085446144\n",
            "INFO:root:Test, MAE: 240, RMSE: 365, MAPE: 384525295274688512\n",
            "Epoch 6: 100%|██████████| 49/49 [00:11<00:00,  4.26it/s]\n",
            "INFO:root:Loss: 101023.227\n",
            "Epoch 7: 100%|██████████| 49/49 [00:11<00:00,  4.22it/s]\n",
            "INFO:root:Loss: 100477.633\n",
            "Epoch 8: 100%|██████████| 49/49 [00:11<00:00,  4.24it/s]\n",
            "INFO:root:Loss: 108965.594\n",
            "Epoch 9: 100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n",
            "INFO:root:Loss: 97342.023\n",
            "Epoch 10: 100%|██████████| 49/49 [00:11<00:00,  4.30it/s]\n",
            "INFO:root:Loss: 95388.812\n",
            "INFO:root:Train, MAE: 267, RMSE: 389, MAPE: 152706754755428352\n",
            "INFO:root:Valid, MAE: 173, RMSE: 273, MAPE: 161083796408500224\n",
            "INFO:root:Test, MAE: 221, RMSE: 338, MAPE: 344967890564284416\n",
            "Epoch 11: 100%|██████████| 49/49 [00:11<00:00,  4.30it/s]\n",
            "INFO:root:Loss: 90767.844\n",
            "Epoch 12: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 90593.391\n",
            "Epoch 13: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 99708.266\n",
            "Epoch 14: 100%|██████████| 49/49 [00:11<00:00,  4.26it/s]\n",
            "INFO:root:Loss: 92274.523\n",
            "Epoch 15: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 91528.406\n",
            "INFO:root:Train, MAE: 256, RMSE: 378, MAPE: 163228033061224448\n",
            "INFO:root:Valid, MAE: 178, RMSE: 279, MAPE: 170139339815124992\n",
            "INFO:root:Test, MAE: 221, RMSE: 338, MAPE: 354895518490427392\n",
            "Epoch 16: 100%|██████████| 49/49 [00:11<00:00,  4.31it/s]\n",
            "INFO:root:Loss: 89089.359\n",
            "Epoch 17: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 90484.945\n",
            "Epoch 18: 100%|██████████| 49/49 [00:11<00:00,  4.30it/s]\n",
            "INFO:root:Loss: 93762.164\n",
            "Epoch 19: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 91867.453\n",
            "Epoch 20: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 92553.859\n",
            "INFO:root:Train, MAE: 231, RMSE: 341, MAPE: 173513912799461376\n",
            "INFO:root:Valid, MAE: 182, RMSE: 283, MAPE: 191907144604319744\n",
            "INFO:root:Test, MAE: 218, RMSE: 330, MAPE: 397704488321810432\n",
            "Epoch 21: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 94475.930\n",
            "Epoch 22: 100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n",
            "INFO:root:Loss: 93544.281\n",
            "Epoch 23: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 96050.734\n",
            "Epoch 24: 100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n",
            "INFO:root:Loss: 95117.578\n",
            "Epoch 25: 100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n",
            "INFO:root:Loss: 109989.258\n",
            "INFO:root:Train, MAE: 245, RMSE: 364, MAPE: 215750415029895168\n",
            "INFO:root:Valid, MAE: 188, RMSE: 293, MAPE: 217268239292432384\n",
            "INFO:root:Test, MAE: 223, RMSE: 339, MAPE: 460250104188633088\n",
            "Epoch 26: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 103025.219\n",
            "Epoch 27: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 92551.328\n",
            "Epoch 28: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 112164.992\n",
            "Epoch 29: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 107893.383\n",
            "Epoch 30: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 120575.266\n",
            "INFO:root:Train, MAE: 282, RMSE: 410, MAPE: 229533549198311424\n",
            "INFO:root:Valid, MAE: 222, RMSE: 332, MAPE: 244987563083825152\n",
            "INFO:root:Test, MAE: 249, RMSE: 364, MAPE: 545682260746043392\n",
            "Epoch 31: 100%|██████████| 49/49 [00:11<00:00,  4.31it/s]\n",
            "INFO:root:Loss: 103979.789\n",
            "Epoch 32: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 100272.141\n",
            "Epoch 33: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 94784.188\n",
            "Epoch 34: 100%|██████████| 49/49 [00:11<00:00,  4.27it/s]\n",
            "INFO:root:Loss: 92714.820\n",
            "Epoch 35: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 87533.219\n",
            "INFO:root:Train, MAE: 242, RMSE: 349, MAPE: 208510749436280832\n",
            "INFO:root:Valid, MAE: 198, RMSE: 286, MAPE: 271671507699105792\n",
            "INFO:root:Test, MAE: 224, RMSE: 324, MAPE: 504585024360677376\n",
            "Epoch 36: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 84777.828\n",
            "Epoch 37: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 86844.000\n",
            "Epoch 38: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 87172.727\n",
            "Epoch 39: 100%|██████████| 49/49 [00:11<00:00,  4.28it/s]\n",
            "INFO:root:Loss: 89230.172\n",
            "Epoch 40: 100%|██████████| 49/49 [00:11<00:00,  4.29it/s]\n",
            "INFO:root:Loss: 87874.547\n",
            "INFO:root:Train, MAE: 240, RMSE: 345, MAPE: 162354127475572736\n"
          ]
        }
      ],
      "source": [
        "# Configure and train model\n",
        "model = model_train(train_dataloader, val_dataloader, config, device, True, test_dataloader, val_vars)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(config['RESULTS_DIR'])"
      ],
      "metadata": {
        "id": "lc3C7Fa6sEGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56TtSHk1_KE-"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "#%reload_ext tensorboard\n",
        "#%tensorboard --logdir ./runs/03-17T17-41-07/ --load_fast=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFCntcgqiWwy"
      },
      "outputs": [],
      "source": [
        "modeling_utils.plot_predictions_vs_gt(pickle_path=config['RESULTS_DIR']+'ygt_ypred.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_rmse, test_mae, test_mape, baseline_rmse, _, _ = eval(model, device, test_dataloader, 'Test', dim_vars, save_predictions=True)\n",
        "print(test_rmse, test_mae, test_mape, baseline_rmse)"
      ],
      "metadata": {
        "id": "Ia3kj2_DmKbE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "trafficPrediction39MLG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "adf2a579d8120a92e1286b98590b288d376803eb678f940738ffad32bae242ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}