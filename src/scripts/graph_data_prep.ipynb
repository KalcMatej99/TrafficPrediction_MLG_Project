{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import preprocessing as preprocessing\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "#Lets start at src location\n",
    "if os.path.exists(\"./src\"):\n",
    "    os.chdir(\"./src\")\n",
    "\n",
    "config = {\n",
    "    \"counter_files_path\"                : \"../data/counters_temporal_data_2023-03-03T09-24-06/\",\n",
    "    \"counters_nontemporal_aggregated\"   : \"../data/counters_non_temporal_aggregated_data.csv\",\n",
    "    \"N_GRAPHS\"                          : 30*24,\n",
    "    \"F_IN\"                              : 7*24,\n",
    "    \"F_OUT\"                             : 7*24,\n",
    "    \"target_col\"                        : \"Sum\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "counters_df = pd.DataFrame()\n",
    "for fname in glob.glob(config[\"counter_files_path\"] + \"*.csv\"):\n",
    "    counter_data = pd.read_csv(fname)\n",
    "    counter_data = preprocessing.fill_gaps(counter_data)\n",
    "    counter_data['Date'] = pd.to_datetime(counter_data['Date']) \n",
    "    counter_data.index = counter_data['Date']\n",
    "    counter_data = counter_data.sort_index(ascending=False)\n",
    "    # We don't need to work with all past data.\n",
    "    # Select enough data points to extract N_GRAPHS with F_IN and F_OUT timepoints\n",
    "    \n",
    "    counter_data = counter_data.iloc[0:(config[\"F_IN\"]+config[\"F_OUT\"]+config[\"N_GRAPHS\"]-1), :]\n",
    "    counter_id = fname.split('/')[-1].split('.csv')[0]\n",
    "\n",
    "    if counters_df.empty:\n",
    "        counters_df = pd.DataFrame(counter_data[config['target_col']])\n",
    "        counters_df.columns = [counter_id]\n",
    "    else:\n",
    "        columns = list(counters_df.columns) + [counter_id]\n",
    "        counters_df = pd.concat([counters_df, counter_data[config['target_col']]], axis=1)\n",
    "        counters_df.columns = columns \n",
    "\n",
    "\n",
    "#Prepare edge_index matrix\n",
    "counters_aggregated = pd.read_csv(config['counters_nontemporal_aggregated'])\n",
    "edge_index, n_node, num_edges = preprocessing.construct_edge_index(counters_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare matrices X [N_GRAPHS, F_IN, N_NODES] and Y [N_GRAPHS, F_OUT, N_NODES] \n",
    "graphs = []\n",
    "for i in range(1, config[\"N_GRAPHS\"]+1):\n",
    "    \n",
    "    g = Data()\n",
    "    g.__num_nodes__ = n_node\n",
    "    g.edge_index = edge_index\n",
    "\n",
    "    train_test_chunk = counters_df.iloc[(-i-(config['F_IN']+config['F_OUT'])):(-i),:]\n",
    "    print(i, train_test_chunk.shape, train_test_chunk.iloc[:config['F_IN'],:].to_numpy().shape)\n",
    "    g.x = torch.FloatTensor(train_test_chunk.iloc[:config['F_IN'],:].to_numpy())\n",
    "    g.y = torch.FloatTensor(train_test_chunk.iloc[config['F_IN']:,:].to_numpy())\n",
    "    graphs += [g]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data: 432\n",
      "Size of validation data: 72\n",
      "Size of test data: 216\n"
     ]
    }
   ],
   "source": [
    "splits = (0.6, 0.1, 0.3)\n",
    "split_train, split_val, _ = splits\n",
    "index_train = int(np.floor(config[\"N_GRAPHS\"]*split_train))\n",
    "index_val = int(index_train + np.floor(config[\"N_GRAPHS\"]*split_val))\n",
    "train_g = graphs[:index_train]\n",
    "val_g = graphs[index_train:index_val]\n",
    "test_g = graphs[index_val:]\n",
    "\n",
    "print(\"Size of train data:\", len(train_g))\n",
    "print(\"Size of validation data:\", len(val_g))\n",
    "print(\"Size of test data:\", len(test_g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trafficPredictionMLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8c3439ea3f1879d024d07407e9519fb000585a35c10aa76c9b6aeca65cc207f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
