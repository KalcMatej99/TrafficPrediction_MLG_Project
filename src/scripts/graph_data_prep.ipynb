{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matejkalc/Documents/FRI/MLG/Project/TrafficPrediction_MLG_Project/trafficPrediction39MLG/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import preprocessing as preprocessing\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "print(torch.__version__)\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Make a tensorboard writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#Lets start at src location\n",
    "if os.path.exists(\"./src\"):\n",
    "    os.chdir(\"./src\")\n",
    "\n",
    "config = {\n",
    "    \"counter_files_path\"                : \"../data/counters_temporal_data_2023-03-03T09-24-06/\",\n",
    "    \"counters_nontemporal_aggregated\"   : \"../data/counters_non_temporal_aggregated_data.csv\",\n",
    "    \"N_GRAPHS\"                          : 30*24,\n",
    "    \"F_IN\"                              : 7*24,\n",
    "    \"F_OUT\"                             : 7*24,\n",
    "    \"target_col\"                        : \"Sum\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "counters_df = pd.DataFrame()\n",
    "for fname in glob.glob(config[\"counter_files_path\"] + \"*.csv\"):\n",
    "    counter_data = pd.read_csv(fname)\n",
    "    counter_data = preprocessing.fill_gaps(counter_data)\n",
    "    counter_data['Date'] = pd.to_datetime(counter_data['Date']) \n",
    "    counter_data.index = counter_data['Date']\n",
    "    counter_data = counter_data.sort_index(ascending=False)\n",
    "    # We don't need to work with all past data.\n",
    "    # Select enough data points to extract N_GRAPHS with F_IN and F_OUT timepoints\n",
    "    \n",
    "    counter_data = counter_data.iloc[0:(config[\"F_IN\"]+config[\"F_OUT\"]+config[\"N_GRAPHS\"]-1), :]\n",
    "    counter_id = fname.split('/')[-1].split('.csv')[0]\n",
    "\n",
    "    if counters_df.empty:\n",
    "        counters_df = pd.DataFrame(counter_data[config['target_col']])\n",
    "        counters_df.columns = [counter_id]\n",
    "    else:\n",
    "        columns = list(counters_df.columns) + [counter_id]\n",
    "        counters_df = pd.concat([counters_df, counter_data[config['target_col']]], axis=1)\n",
    "        counters_df.columns = columns \n",
    "\n",
    "\n",
    "#Prepare edge_index matrix\n",
    "counters_aggregated = pd.read_csv(config['counters_nontemporal_aggregated'])\n",
    "edge_index, n_node, num_edges = preprocessing.construct_edge_index(counters_aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare matrices X [N_GRAPHS, N_NODES, F_IN] and Y [N_GRAPHS, N_NODES, F_OUT] \n",
    "graphs = []\n",
    "for i in range(1, config[\"N_GRAPHS\"]+1):\n",
    "    \n",
    "    g = Data()\n",
    "    g.__num_nodes__ = n_node\n",
    "    g.edge_index = edge_index\n",
    "\n",
    "    train_test_chunk = counters_df.iloc[(-i-(config['F_IN']+config['F_OUT'])):(-i),:]\n",
    "    g.x = torch.FloatTensor(train_test_chunk.iloc[:config['F_IN'],:].to_numpy().T)\n",
    "    g.y = torch.FloatTensor(train_test_chunk.iloc[config['F_IN']:,:].to_numpy().T)\n",
    "    graphs += [g]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data: 432\n",
      "Size of validation data: 72\n",
      "Size of test data: 216\n"
     ]
    }
   ],
   "source": [
    "splits = (0.6, 0.1, 0.3)\n",
    "split_train, split_val, _ = splits\n",
    "index_train = int(np.floor(config[\"N_GRAPHS\"]*split_train))\n",
    "index_val = int(index_train + np.floor(config[\"N_GRAPHS\"]*split_val))\n",
    "train_g = graphs[:index_train]\n",
    "val_g = graphs[index_train:index_val]\n",
    "test_g = graphs[index_val:]\n",
    "\n",
    "print(\"Size of train data:\", len(train_g))\n",
    "print(\"Size of validation data:\", len(val_g))\n",
    "print(\"Size of test data:\", len(test_g))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "class ST_GAT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Spatio-Temporal Graph Attention Network as presented in https://ieeexplore.ieee.org/document/8903252\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, n_nodes, heads=8, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the ST-GAT model\n",
    "        :param in_channels Number of input channels\n",
    "        :param out_channels Number of output channels\n",
    "        :param n_nodes Number of nodes in the graph\n",
    "        :param heads Number of attention heads to use in graph\n",
    "        :param dropout Dropout probability on output of Graph Attention Network\n",
    "        \"\"\"\n",
    "        super(ST_GAT, self).__init__()\n",
    "        self.n_pred = out_channels\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        self.n_preds = 9\n",
    "        lstm1_hidden_size = 32\n",
    "        lstm2_hidden_size = 128\n",
    "\n",
    "        # single graph attentional layer with 8 attention heads\n",
    "        self.gat = GATConv(in_channels=in_channels, out_channels=in_channels,\n",
    "            heads=heads, dropout=0, concat=False)\n",
    "\n",
    "        # add two LSTM layers\n",
    "        self.lstm1 = torch.nn.LSTM(input_size=self.n_nodes, hidden_size=lstm1_hidden_size, num_layers=1)\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                torch.nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "        self.lstm2 = torch.nn.LSTM(input_size=lstm1_hidden_size, hidden_size=lstm2_hidden_size, num_layers=1)\n",
    "        for name, param in self.lstm1.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                torch.nn.init.constant_(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "\n",
    "        # fully-connected neural network\n",
    "        self.linear = torch.nn.Linear(lstm2_hidden_size, self.n_nodes*self.n_pred)\n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "    def forward(self, data, device):\n",
    "        \"\"\"\n",
    "        Forward pass of the ST-GAT model\n",
    "        :param data Data to make a pass on\n",
    "        :param device Device to operate on\n",
    "        \"\"\"\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # apply dropout\n",
    "        if device == 'cpu':\n",
    "            x = torch.FloatTensor(x)\n",
    "        else:\n",
    "            x = torch.cuda.FloatTensor(x)\n",
    "\n",
    "        x = self.gat(x, edge_index)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        # RNN: 2 LSTM\n",
    "        batch_size = data.num_graphs\n",
    "        n_node = int(data.num_nodes/batch_size)\n",
    "        x = torch.reshape(x, (batch_size, n_node, data.num_features))\n",
    "        x = torch.movedim(x, 2, 0)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "\n",
    "        x = torch.squeeze(x[-1, :, :])\n",
    "        x = self.linear(x)\n",
    "\n",
    "        s = x.shape\n",
    "        x = torch.reshape(x, (s[0], self.n_nodes, self.n_pred))\n",
    "        x = torch.reshape(x, (s[0]*self.n_nodes, self.n_pred))\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Make a tensorboard writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def model_train(train_dataloader, val_dataloader, config, device):\n",
    "    \"\"\"\n",
    "    Train the ST-GAT model. Evaluate on validation dataset as you go.\n",
    "    :param train_dataloader Data loader of training dataset\n",
    "    :param val_dataloader Dataloader of val dataset\n",
    "    :param config configuration to use\n",
    "    :param device Device to evaluate on\n",
    "    \"\"\"\n",
    "\n",
    "    # Make the model. Each datapoint in the graph is 228x12: N x F (N = # nodes, F = time window)\n",
    "    model = ST_GAT(in_channels=config['F_IN'], out_channels=config['F_OUT'], n_nodes=config['N_NODE'], dropout=config['DROPOUT'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['INITIAL_LR'], weight_decay=config['WEIGHT_DECAY'])\n",
    "    loss_fn = torch.nn.MSELoss\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # For every epoch, train the model on training dataset. Evaluate model on validation dataset\n",
    "    for epoch in range(config['EPOCHS']):\n",
    "        loss = train(model, device, train_dataloader, optimizer, loss_fn, epoch)\n",
    "        print(f\"Loss: {loss:.3f}\")\n",
    "        if epoch % 5 == 0:\n",
    "            train_mae, train_rmse, train_mape, _, _ = eval(model, device, train_dataloader, 'Train')\n",
    "            val_mae, val_rmse, val_mape, _, _ = eval(model, device, val_dataloader, 'Valid')\n",
    "            writer.add_scalar(f\"MAE/train\", train_mae, epoch)\n",
    "            writer.add_scalar(f\"RMSE/train\", train_rmse, epoch)\n",
    "            writer.add_scalar(f\"MAPE/train\", train_mape, epoch)\n",
    "            writer.add_scalar(f\"MAE/val\", val_mae, epoch)\n",
    "            writer.add_scalar(f\"RMSE/val\", val_rmse, epoch)\n",
    "            writer.add_scalar(f\"MAPE/val\", val_mape, epoch)\n",
    "\n",
    "    writer.flush()\n",
    "    # Save the model\n",
    "    timestr = time.strftime(\"%m-%d-%H%M%S\")\n",
    "    torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss,\n",
    "            }, os.path.join(config[\"CHECKPOINT_DIR\"], f\"model_{timestr}.pt\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_test(model, test_dataloader, device, config):\n",
    "    \"\"\"\n",
    "    Test the ST-GAT model\n",
    "    :param test_dataloader Data loader of test dataset\n",
    "    :param device Device to evaluate on\n",
    "    \"\"\"\n",
    "    _, _, _, y_pred, y_truth = eval(model, device, test_dataloader, 'Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x, mean, std):\n",
    "    return (x - mean) / std\n",
    "def un_z_score(x_normed, mean, std):\n",
    "    return x_normed * std  + mean\n",
    "def MAPE(v, v_):\n",
    "    return torch.mean(torch.abs((v_ - v)) /(v + 1e-15) * 100)\n",
    "def RMSE(v, v_):\n",
    "    return torch.sqrt(torch.mean((v_ - v) ** 2))\n",
    "def MAE(v, v_):\n",
    "    return torch.mean(torch.abs(v_ - v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, device, dataloader, type=''):\n",
    "    \"\"\"\n",
    "    Evaluation function to evaluate model on data\n",
    "    :param model Model to evaluate\n",
    "    :param device Device to evaluate on\n",
    "    :param dataloader Data loader\n",
    "    :param type Name of evaluation type, e.g. Train/Val/Test\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    mae = 0\n",
    "    rmse = 0\n",
    "    mape = 0\n",
    "    n = 0\n",
    "\n",
    "    # Evaluate model on all data\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch, device)\n",
    "            truth = batch.y.view(pred.shape)\n",
    "            if i == 0:\n",
    "                y_pred = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
    "                y_truth = torch.zeros(len(dataloader), pred.shape[0], pred.shape[1])\n",
    "            #truth = un_z_score(truth, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
    "            #pred = un_z_score(pred, dataloader.dataset.mean, dataloader.dataset.std_dev)\n",
    "            y_pred[i, :pred.shape[0], :] = pred\n",
    "            y_truth[i, :pred.shape[0], :] = truth\n",
    "            rmse += RMSE(truth, pred)\n",
    "            mae += MAE(truth, pred)\n",
    "            mape += MAPE(truth, pred)\n",
    "            n += 1\n",
    "    rmse, mae, mape = rmse / n, mae / n, mape / n\n",
    "\n",
    "    print(f'{type}, MAE: {mae}, RMSE: {rmse}, MAPE: {mape}')\n",
    "\n",
    "    #get the average score for each metric in each batch\n",
    "    return rmse, mae, mape, y_pred, y_truth\n",
    "\n",
    "def train(model, device, dataloader, optimizer, loss_fn, epoch):\n",
    "    \"\"\"\n",
    "    Evaluation function to evaluate model on data\n",
    "    :param model Model to evaluate\n",
    "    :param device Device to evaluate on\n",
    "    :param dataloader Data loader\n",
    "    :param optimizer Optimizer to use\n",
    "    :param loss_fn Loss function\n",
    "    :param epoch Current epoch\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for _, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch}\")):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = torch.squeeze(model(batch, device))\n",
    "        loss = loss_fn()(y_pred.float(), torch.squeeze(batch.y).float())\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 9/9 [00:06<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 641392.750\n",
      "Train, MAE: 590.2749633789062, RMSE: 830.9781494140625, MAPE: 57905986404352.0\n",
      "Valid, MAE: 590.8264770507812, RMSE: 818.2146606445312, MAPE: 90645821652992.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9/9 [00:05<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 641331.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9/9 [00:05<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 641066.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 9/9 [00:05<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 640308.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 9/9 [00:05<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 639608.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9/9 [00:06<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 638916.875\n",
      "Train, MAE: 588.205322265625, RMSE: 829.4130249023438, MAPE: 1772169809887232.0\n",
      "Valid, MAE: 588.7334594726562, RMSE: 816.6260986328125, MAPE: 2537759809994752.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 9/9 [00:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 638251.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9/9 [00:06<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 637631.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 9/9 [00:05<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 637050.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9/9 [00:06<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 636499.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 9/9 [00:06<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 635972.812\n",
      "Train, MAE: 585.8417358398438, RMSE: 827.6145629882812, MAPE: 2928136903720960.0\n",
      "Valid, MAE: 586.3336791992188, RMSE: 814.801025390625, MAPE: 4183457596964864.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 635463.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 9/9 [00:06<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 634966.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 9/9 [00:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 634481.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 9/9 [00:07<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 634005.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 9/9 [00:08<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 633537.188\n",
      "Train, MAE: 583.8745727539062, RMSE: 826.115966796875, MAPE: 3967258070089728.0\n",
      "Valid, MAE: 584.3356323242188, RMSE: 813.27978515625, MAPE: 5663894706061312.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 633075.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 9/9 [00:07<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 632620.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 9/9 [00:06<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 632170.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 9/9 [00:06<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 631724.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 9/9 [00:07<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 631283.000\n",
      "Train, MAE: 582.0499267578125, RMSE: 824.7247924804688, MAPE: 4912820353236992.0\n",
      "Valid, MAE: 582.4815673828125, RMSE: 811.8677978515625, MAPE: 7013072782753792.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 9/9 [00:07<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 630845.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 9/9 [00:06<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 630409.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 9/9 [00:06<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 629977.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 9/9 [00:07<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 629548.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 9/9 [00:07<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 629122.250\n",
      "Train, MAE: 580.298095703125, RMSE: 823.38818359375, MAPE: 5810758965264384.0\n",
      "Valid, MAE: 580.700927734375, RMSE: 810.510986328125, MAPE: 8294221798703104.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 9/9 [00:06<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 628697.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 9/9 [00:07<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 628275.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 9/9 [00:07<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 627854.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 9/9 [00:06<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 627436.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 9/9 [00:06<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 627019.750\n",
      "Train, MAE: 578.5923461914062, RMSE: 822.0852661132812, MAPE: 6676434856706048.0\n",
      "Valid, MAE: 578.9664306640625, RMSE: 809.1885986328125, MAPE: 9529396410974208.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|██████████| 9/9 [00:06<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 626604.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 626191.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|██████████| 9/9 [00:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 625779.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 9/9 [00:06<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 625368.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|██████████| 9/9 [00:07<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 624959.188\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def distance_to_weight(W, sigma2=0.1, epsilon=0.5, gat_version=False):\n",
    "    \"\"\"\"\n",
    "    Given distances between all nodes, convert into a weight matrix\n",
    "    :param W distances\n",
    "    :param sigma2 User configurable parameter to adjust sparsity of matrix\n",
    "    :param epsilon User configurable parameter to adjust sparsity of matrix\n",
    "    :param gat_version If true, use 0/1 weights with self loops. Otherwise, use float\n",
    "    \"\"\"\n",
    "    n = W.shape[0]\n",
    "    W = W / 10000.\n",
    "    W2, W_mask = W * W, np.ones([n, n]) - np.identity(n)\n",
    "    # refer to Eq.10\n",
    "    W = np.exp(-W2 / sigma2) * (np.exp(-W2 / sigma2) >= epsilon) * W_mask\n",
    "\n",
    "    # If using the gat version of this, round to 0/1 and include self loops\n",
    "    if gat_version:\n",
    "        W[W>0] = 1\n",
    "        W += np.identity(n)\n",
    "\n",
    "    return W\n",
    "\n",
    "# Constant config to use throughout\n",
    "config = {\n",
    "    'BATCH_SIZE': 50,\n",
    "    'EPOCHS': 60,\n",
    "    'WEIGHT_DECAY': 5e-5,\n",
    "    'INITIAL_LR': 3e-4,\n",
    "    'CHECKPOINT_DIR': '../runs',\n",
    "    'DROPOUT': 0.2,\n",
    "    # If false, use GCN paper weight matrix, if true, use GAT paper weight matrix\n",
    "    'USE_GAT_WEIGHTS': False,\n",
    "    \"counter_files_path\"                : \"../data/counters_temporal_data_2023-03-03T09-24-06/\",\n",
    "    \"counters_nontemporal_aggregated\"   : \"../data/counters_non_temporal_aggregated_data.csv\",\n",
    "    \"N_GRAPHS\"                          : 30*24,\n",
    "    \"F_IN\"                              : 7*24,\n",
    "    \"F_OUT\"                             : 7*24,\n",
    "    \"target_col\"                        : \"Sum\"\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(train_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "val_dataloader = DataLoader(val_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "test_dataloader = DataLoader(test_g, batch_size=config['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "# Get gpu if you can\n",
    "device = 'cpu'#cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Configure and train model\n",
    "config['N_NODE'] = n_node\n",
    "model = model_train(train_dataloader, val_dataloader, config, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trafficPrediction39MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "adf2a579d8120a92e1286b98590b288d376803eb678f940738ffad32bae242ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
